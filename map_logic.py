import pandas as pd
import time
import requests
import urllib.parse
import re
import folium
from folium.plugins import HeatMap
import streamlit as st
import sqlite3
import streamlit.components.v1 as components

scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
# ã‚¸ã‚ªã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–
path = r"C:\Users\Owner\Desktop\00_zenkoku_all_20250930.csv"

def normalize_address(addr):
    # å…¨è§’æ•°å­—ãƒ»è¨˜å·ã‚’åŠè§’ã«å¤‰æ›
    import unicodedata
    return unicodedata.normalize('NFKC', addr)

def deal(path, year_range=(2023, 2024)):
   # CSVèª­ã¿è¾¼ã¿ï¼ˆShift-JISç‰ˆï¼‰
   path = path
   df = pd.read_csv(path, encoding='cp932', header=None,dtype=str)

   # å¿…è¦ãªã‚«ãƒ©ãƒ åã‚’å®šç¾©ï¼ˆé …ç•ªã«å¯¾å¿œï¼‰
   df.columns =  ["sequenceNumber","corporateNumber","process1","correct","updateDate",
                  "changeDate","name","nameImageId","process2","prefectureName",
                  "cityName","streetNumber","Address ImageId","prefectureCode",
                  "cityCode","postCode","addressOutside","AddressOutside ImageId",
                  "closeDate","closeCause","Successor CorporateNumber",
                  "changeCause","assignmentDate","Latest(0:old.1:latest)",
                  "enName","enPrefectureName","enCityName","enAddressOutside","furigana","hihyoji"
                  ]

   #print(df['process_code'])
   # å‡¦ç†åŒºåˆ†ã‚³ãƒ¼ãƒ‰ã®æ„å‘³ï¼ˆå‚è€ƒï¼‰
   process_map = {
      '01': 'æ–°è¦',
      '11': 'å•†å·å¤‰æ›´',
      '12': 'æ‰€åœ¨åœ°å¤‰æ›´',
      '21': 'ç™»è¨˜é–‰é–ï¼ˆå»ƒæ­¢ãƒ»åˆä½µï¼‰',
      '22': 'ç™»è¨˜å¾©æ´»',
      '71': 'å¸ååˆä½µ',
      '81': 'å•†å·æŠ¹æ¶ˆ',
      '99': 'å‰Šé™¤'
   }
   legal_entity_codes = {
    "101": "å›½ã®æ©Ÿé–¢ï¼ˆè¡Œæ”¿æ©Ÿé–¢ãƒ»å›½ç«‹å¤§å­¦æ³•äººãªã©ï¼‰",
    "201": "åœ°æ–¹å…¬å…±å›£ä½“ï¼ˆéƒ½é“åºœçœŒã€å¸‚ç”ºæ‘ã€ç‰¹åˆ¥åŒºãªã©ï¼‰",
    "301": "æ ªå¼ä¼šç¤¾",
    "302": "æœ‰é™ä¼šç¤¾ï¼ˆç‰¹ä¾‹æœ‰é™ä¼šç¤¾ï¼‰",
    "303": "åˆåä¼šç¤¾",
    "304": "åˆè³‡ä¼šç¤¾",
    "305": "åˆåŒä¼šç¤¾",
    "399": "ãã®ä»–ã®è¨­ç«‹ç™»è¨˜æ³•äººï¼ˆä¸€èˆ¬ç¤¾å›£æ³•äººãªã©ï¼‰",
    "401": "å¤–å›½ä¼šç¤¾ï¼ˆå¤–å›½æ³•ã«åŸºã¥ãæ³•äººï¼‰",
    "499": "ãã®ä»–ï¼ˆåŒºåˆ†ä¸èƒ½ãªæ³•äººï¼‰"
}

   # å‡¦ç†åŒºåˆ†ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆè¨­ç«‹ãƒ»å»ƒæ­¢ãƒ»åˆä½µï¼‰
   target_codes = ['21']
   filtered = df[df['process1'].isin(target_codes)].copy()
   filtered['event'] = filtered['process1'].map(process_map)
   
   # æ—¥ä»˜ã¨å¹´æŠ½å‡º
   filtered['changeDate'] = pd.to_datetime(filtered['changeDate'], errors='coerce')
   filtered['closed_year'] = filtered['changeDate'].dt.year

   # ä½æ‰€çµåˆï¼ˆéƒµä¾¿ç•ªå·ä»˜ãï¼‰
   filtered['full_address'] =filtered['prefectureName'].fillna('') + \
                            filtered['cityName'].fillna('') + \
                            filtered['streetNumber'].fillna('')
                             
   result = filtered[[
      "corporateNumber", 'name', 'event', "changeDate", "closed_year",'full_address',]]
   result = result[(result['closed_year'] >= year_range[0]) &(result['closed_year'] <= year_range[1])]
   if result.empty:
    print("âš ï¸ æŒ‡å®šæœŸé–“ã«è©²å½“ã™ã‚‹å»ƒæ¥­ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    return


   # è¡¨ç¤ºï¼ˆã¾ãŸã¯CSVä¿å­˜ï¼‰
   #print(result)
   # result.to_csv('filtered_corp_events.csv', index=False, encoding='utf-8')
   return result

def geocode_gsi(address):
    # éƒµä¾¿ç•ªå·é™¤å»ï¼ˆã€’123-4567 â†’ å‰Šé™¤ï¼‰
    address = re.sub(r'ã€’\d{3}-\d{4}', '', address)

    # ç©ºç™½é™¤å»ï¼‹URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    encoded = urllib.parse.quote(address.replace(' ', ''))

    # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    url = f"https://msearch.gsi.go.jp/address-search/AddressSearch?q={encoded}"
    try:
        response = requests.get(url)
        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

        if response.status_code == 200 and response.text.strip():
            data = response.json()
            if data:
                lat = data[0]["geometry"]["coordinates"][1]
                lon = data[0]["geometry"]["coordinates"][0]
                return lat, lon
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
    return None, None

def makesqldb(path):
    df = pd.read_csv(path, encoding='cp932', header=None, dtype=str)
    st.write("CSVèª­ã¿è¾¼ã¿è¡Œæ•°:", len(df))

    df.columns = [
        "sequenceNumber", "corporateNumber", "process1", "correct", "updateDate",
        "changeDate", "name", "nameImageId", "process2", "prefectureName",
        "cityName", "streetNumber", "Address ImageId", "prefectureCode",
        "cityCode", "postCode", "addressOutside", "AddressOutside ImageId",
        "closeDate", "closeCause", "Successor CorporateNumber",
        "changeCause", "assignmentDate", "Latest(0:old.1:latest)",
        "enName", "enPrefectureName", "enCityName", "enAddressOutside", "furigana", "hihyoji"
    ]

    conn = sqlite3.connect("houjin.db")
    df.to_sql("houjin", conn, if_exists="replace", index=False)
    conn.commit()
    conn.close()

    conn = sqlite3.connect("houjin.db")
    count_df = pd.read_sql("SELECT COUNT(*) FROM houjin", conn)
    st.write("DBç™»éŒ²ä»¶æ•°:", count_df.iloc[0, 0])
    conn.close()

    return 

def implementsql(db_path):
    # GitHubä¸Šã®JIS X 0401éƒ½é“åºœçœŒãƒªã‚¹ãƒˆä½œæˆ
    url = "https://raw.githubusercontent.com/HirMtsd/Code/refs/heads/master/JISX0401/Prefecture_list.json"
    response = requests.get(url)
    pref_data = response.json()["prefectures"]

    # éƒ½é“åºœçœŒå â†’ ã‚³ãƒ¼ãƒ‰ã®è¾æ›¸ä½œæˆ
    pref_options = {item["name"]: item["code"] for item in pref_data}
    
    # Streamlitã§éƒ½é“åºœçœŒåã‚’é¸æŠ
    selected_pref_name = st.selectbox("éƒ½é“åºœçœŒã‚’é¸æŠ", list(pref_options.keys()))
    selected_pref_code = pref_options[selected_pref_name]

    # GitHubä¸Šã®å¸‚ç”ºæ‘JSONï¼ˆJIS X 0402ï¼‰
    url_city = "https://raw.githubusercontent.com/HirMtsd/Code/refs/heads/master/JISX0402/City_list.json"
    response_city = requests.get(url_city)
    city_data = response_city.json()["cities"]

    # å¸‚ç”ºæ‘å â†’ ã‚³ãƒ¼ãƒ‰ã®è¾æ›¸ï¼ˆé¸æŠã•ã‚ŒãŸéƒ½é“åºœçœŒã®ã¿ï¼‰
    city_options = {
        item["name"]: {"pref_code": item["pref_code"], "city_code": item["city_code"]}
        for item in city_data if item["pref_code"] == selected_pref_code}

    #æœŸå¾…å€¤ã€€{"åŠç”°å¸‚": {"pref_code": "23", "city_code": "205"},"åå¤å±‹å¸‚": {"pref_code": "23", "city_code": "101"},"è±Šç”°å¸‚": {"pref_code": "23", "city_code": "239"},...}


    # Streamlitã§å¸‚ç”ºæ‘åã‚’é¸æŠ
    selected_city_name = st.selectbox("å¸‚ç”ºæ‘ã‚’é¸æŠ", list(city_options.keys()))
    selected_codes = city_options[selected_city_name]

    conn = sqlite3.connect(db_path)
    #conn = sqlite3.connect("data/houjin.db")  # ãƒªãƒã‚¸ãƒˆãƒªå†…ã®dataãƒ•ã‚©ãƒ«ãƒ€

    # ç·¯åº¦ãƒ»çµŒåº¦ãŒæœªç™»éŒ²ã®æ³•äººãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    query = f"""
    SELECT *
    FROM houjin
    WHERE (lat IS NULL OR lon IS NULL)
    AND prefectureCode = '{selected_codes["pref_code"]}'
    AND cityCode = '{selected_codes["city_code"]}'
    """


    #df = pd.read_sql(query, conn)
    try:
        df = pd.read_sql(query, conn)
    except Exception as e:
        st.error(f"SQLèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()
    #print(df['process_code'])
    # å‡¦ç†åŒºåˆ†ã‚³ãƒ¼ãƒ‰ã®æ„å‘³ï¼ˆå‚è€ƒï¼‰
    process_map = {
      '01': 'æ–°è¦',
      '11': 'å•†å·å¤‰æ›´',
      '12': 'æ‰€åœ¨åœ°å¤‰æ›´',
      '21': 'ç™»è¨˜é–‰é–ï¼ˆå»ƒæ­¢ãƒ»åˆä½µï¼‰',
      '22': 'ç™»è¨˜å¾©æ´»',
      '71': 'å¸ååˆä½µ',
      '81': 'å•†å·æŠ¹æ¶ˆ',
      '99': 'å‰Šé™¤'
    }

    legal_entity_codes = {
    "101": "å›½ã®æ©Ÿé–¢ï¼ˆè¡Œæ”¿æ©Ÿé–¢ãƒ»å›½ç«‹å¤§å­¦æ³•äººãªã©ï¼‰",
    "201": "åœ°æ–¹å…¬å…±å›£ä½“ï¼ˆéƒ½é“åºœçœŒã€å¸‚ç”ºæ‘ã€ç‰¹åˆ¥åŒºãªã©ï¼‰",
    "301": "æ ªå¼ä¼šç¤¾",
    "302": "æœ‰é™ä¼šç¤¾ï¼ˆç‰¹ä¾‹æœ‰é™ä¼šç¤¾ï¼‰",
    "303": "åˆåä¼šç¤¾",
    "304": "åˆè³‡ä¼šç¤¾",
    "305": "åˆåŒä¼šç¤¾",
    "399": "ãã®ä»–ã®è¨­ç«‹ç™»è¨˜æ³•äººï¼ˆä¸€èˆ¬ç¤¾å›£æ³•äººãªã©ï¼‰",
    "401": "å¤–å›½ä¼šç¤¾ï¼ˆå¤–å›½æ³•ã«åŸºã¥ãæ³•äººï¼‰",
    "499": "ãã®ä»–ï¼ˆåŒºåˆ†ä¸èƒ½ãªæ³•äººï¼‰"
    }

    # å‡¦ç†åŒºåˆ†ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆè¨­ç«‹ãƒ»å»ƒæ­¢ãƒ»åˆä½µï¼‰
    target_codes = ['21']
    filtered = df[df['process1'].isin(target_codes)].copy()
    filtered['event'] = filtered['process1'].map(process_map)

    # ä½æ‰€çµåˆï¼ˆéƒ½é“åºœçœŒï¼‹å¸‚ç”ºæ‘ï¼‹ç•ªåœ°ï¼‰
    df["full_address"] = (
        df["prefectureName"].fillna("") +
        df["cityName"].fillna("") +
        df["streetNumber"].fillna("")
    )
    df["assignmentDate"] = pd.to_datetime(df["assignmentDate"], errors="coerce")
    df["changeDate"] = pd.to_datetime(df["changeDate"], errors="coerce")

    # ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
    def classify_lifespan(days):
        if pd.isna(days):
            return "ä¸æ˜"
        elif days < 180:
            return "âš ï¸çŸ­å‘½ï¼"
        elif days >= 3650:
            return "ğŸ†é•·å¯¿ï¼"
        else:
            return "æ™®é€š"

    # 3. å¯¿å‘½è¨ˆç®—
    df["lifespan_days"] = (df["changeDate"] - df["assignmentDate"]).dt.days
    df["lifespan_years"] = (df["lifespan_days"] / 365).round(1)

    # 4. ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼ˆçŸ­å‘½ãƒ»é•·å¯¿ãªã©ï¼‰
    df["lifespan_comment"] = df["lifespan_days"].apply(classify_lifespan)
    df["lifespan_text"] = (
        "å­˜ç¶šæœŸé–“ï¼š" +
        df["lifespan_days"].astype(str) + "æ—¥ï¼ˆç´„" +
        df["lifespan_years"].astype(str) + "å¹´ï¼‰ " +
        df["lifespan_comment"]
    )

    conn.close()
    return df

def show_map(df):
    m = folium.Map(location=[df["lat"].mean(), df["lon"].mean()], zoom_start=10)
    for _, row in df.iterrows():
        folium.Marker(
            location=[row["lat"], row["lon"]],
            popup=f"{row['name']}<br>{row['lifespan_text']}",
            tooltip=row["full_address"]
        ).add_to(m)
    return m

def main2(db_path):
    import time

    # ç·¯åº¦çµŒåº¦ã®åˆ—ã‚’è¿½åŠ 
    latitudes = []
    longitudes = []

    # æ³•äººãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆå¯¿å‘½æƒ…å ±ä»˜ãï¼‰
    filtered = implementsql(db_path)
    filtered = filtered.iloc[:10, :]  # ã‚µãƒ³ãƒ—ãƒ«åˆ¶é™

    # ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆåœ°ç†é™¢APIï¼‰
    for addr in filtered['full_address']:
        normalized = normalize_address(addr)
        lat, lon = geocode_gsi(normalized)
        latitudes.append(lat)
        longitudes.append(lon)
        time.sleep(1.1)  # APIåˆ¶é™å¯¾ç­–

    # ç·¯åº¦ãƒ»çµŒåº¦ã‚’DataFrameã«è¿½åŠ 
    filtered.loc[:, 'lat'] = latitudes
    filtered.loc[:, 'lon'] = longitudes

    # åœ°å›³æç”»ã«ä½¿ã†åˆ—ã ã‘æŠ½å‡º
    geo_df = filtered[["corporateNumber", 'name', 'event', "changeDate", "closed_year",'full_address', 'lat', 'lon',"lifespan_years","lifespan_days"]]

    # æ¬ æã‚’é™¤å¤–ï¼ˆç·¯åº¦çµŒåº¦ãŒå–å¾—ã§ããªã‹ã£ãŸè¡Œã‚’é™¤ãï¼‰
    geo_df = geo_df.dropna(subset=['lat', 'lon'])
    m = show_map(geo_df)
    m.save("corp_map.html")  # HTMLã¨ã—ã¦ä¸€æ™‚ä¿å­˜
    components.html(m._repr_html_(), height=600)
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯è¡¨ç¤º
    with open("corp_map.html", "rb") as f:
        st.download_button(
            label="ğŸ“¥ åœ°å›³ã‚’HTMLã§ä¿å­˜",
            data=f,
            file_name="corp_map.html",
            mime="text/html"
        )

    return 

